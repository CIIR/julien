import org.lemurproject.galago.core.index.disk.DiskIndex
import org.lemurproject.galago.core.index.Index
import org.lemurproject.galago.core.retrieval.iterator.MovableLengthsIterator
import org.lemurproject.galago.core.retrieval.query.{StructuredQuery, Node}
import org.lemurproject.galago.core.index.disk.PositionIndexReader
import scala.collection.mutable.PriorityQueue
import org.lemurproject.galago.core.retrieval.iterator.MovableIterator
import org.lemurproject.galago.core.parse.{Document,Tag,TagTokenizer}
import org.lemurproject.galago.tupleflow.Utility
import org.lemurproject.galago.tupleflow.Parameters
import scala.collection.mutable.ListBuffer
import scala.collection.mutable.LinkedHashMap
import scala.collection.mutable.HashMap
import scala.collection.JavaConversions._
import scala.collection.immutable.Set
import scala.collection.Map

import java.net.URL
import scala.io.Source._

// Hack for now to load some common definitions
import GalagoBridging._

object Models {
  type RetrievalFunction = () => Unit

  def main(args: Array[String]) : Unit = {
    models.get(args(0)) match {
      case Some(f: RetrievalFunction) => f()
      case None => {
        Console.printf("I don't know what '%s'\n", args(0))
      }
    }
  }

  val models = HashMap[String, RetrievalFunction]()
  models("bow") = () => {
    // Query prep
    val query = "new york city"
    val queryNodes = bowNodes(query)

    val aquaint = Sources.get('aquaint)
    val lengths = aquaint.getLengthsIterator
    val nodeMap = LinkedHashMap[Node, java.lang.Object]()
    for (n <- queryNodes) { nodeMap.update(n,
      aquaint.getIterator(n).asInstanceOf[PositionIndexReader#TermCountIterator])
    }

    val iterators = nodeMap.values.toList.map(obj2movableIt(_))

    // Create the scoring functions that map over the iterators
    val scorers = unigrams(nodeMap, aquaint, lengths)

    // Scoring loop
    val resultQueue = standardScoringLoop(scorers, iterators, lengths)

    // Get doc names and print
    printResults(resultQueue, aquaint)
  }

  models("sdm") = () => {
    // Query prep
    val query = "new york city"
    val queryNodes = bowNodes(query, "extents")
    val aquaint = Sources.get('aquaint)
    val lengths = aquaint.getLengthsIterator
    val nodeMap = LinkedHashMap[Node, java.lang.Object]()
    for (n <- queryNodes) { nodeMap.update(n,
      aquaint.getIterator(n).asInstanceOf[PositionIndexReader#TermExtentIterator])
    }

    val iterators = nodeMap.values.toList.map(obj2movableIt(_))

    // Make the scorers - more complex here because we use unigrams,
    // phrases, and windows, and each one is under a combining scorer.
    val scorers = List(
      ParameterizedScorer(unigrams(nodeMap, aquaint, lengths), 0.8),
      ParameterizedScorer(orderedWindows(nodeMap, aquaint, lengths), 0.15),
      ParameterizedScorer(unorderedWindows(nodeMap, aquaint, lengths, 8), 0.05)
    )

    // Scoring loop
    val resultQueue = standardScoringLoop(scorers, iterators, lengths)

    // Get doc names and print
    printResults(resultQueue, aquaint)
  }

  models("pqm") = () => {
    // components of our feature functions
    // Hold external references to the feature weights so we can tune quickly
    val weightTable = HashMap[String, List[Double]]()
    val tokenizer = new TagTokenizer()
    val stopwords = Stopwords.inquery

    // Define our feature function as a list of functions generated by this function
    def generateFeatures(
      qN: Node,
      nodeMap: Map[Node, java.lang.Object],
      mainIndex: Index,
      auxIndex: Index) : List[FeatureFunction] = {
      val feats = ListBuffer[FeatureFunction]()

      // Calculate these once since they're static
      val dummy = new Parameters
      val term = qN.getDefaultParameter
      val auxLengths = auxIndex.getLengthsIterator
      val auxSize = collectionLength(auxLengths)
      val auxNumDocs = numDocuments(auxLengths)

      // First one is to look up the cf in the title section of
      // the auxiliary index
      var f = () => {
        val node = formatNode(term, field = "title")
        val cCount = collectionCount(node, auxIndex)
        cCount.toDouble / auxSize
      }
      feats += f

      // Now do one for idf
      f = () => {
        val node = formatNode(term, field = "title")
        val dCount = documentCount(node, auxIndex)
        scala.math.log(auxNumDocs.toDouble / (dCount + 0.5))
      }
      feats += f

      // Time to get weird - every time the word occurs more than 10 times
      // in a doc, pull up the document, and determine the number of times
      // it's capitalized IN the raw text
      f = () => {
        // Grab the iterator for this feature and the context
        val it = nodeMap(qN).asInstanceOf[TCI]
        val count = it.count
        val id = it.getContext.document
        if (count > 10) {
          val doc = mainIndex.getDocument(mainIndex.getName(id), dummy)
          val matchPattern = String.format("""(?iu)\W%s\W""", term).r
          val total = matchPattern.findAllIn(doc.text).map { data =>
            if (data.size > 1 && data(0).isUpper) 1 else 0
          }.sum
          total
        } else {
          0.1
        }
      }
      feats += f

      // Finally - even worse - if the word occurs more than 20 times in a doc,
      // pull the doc, get all the terms in the title tag. Use those to
      // ping Google and look at the results page. Return the proportion of words
      // that are not stopwords
      f = () => {

        val it = nodeMap(qN).asInstanceOf[TCI]
        val count = it.count
        val id = it.getContext.document
        var returnValue = 0.0001
        if (count > 10) {
          Console.printf("F4: %s\n", term)
          val doc = mainIndex.getDocument(mainIndex.getName(id), dummy)
          if (doc.terms == null || doc.terms.size == 0) tokenizer.tokenize(doc)
          // Get the title tag and grab the content from the term vector
          val titleQuery = doc.tags.find(_.name.toLowerCase == "title") match {
            case None => ""
            case Some(t: Tag) =>
              doc.terms.slice(t.begin, t.end).toSet.mkString("+")
          }
          Console.printf("|TQ| = %d\n", titleQuery.size)

          if (titleQuery.size > 0) {
            // Now let's hit up Bing (Google's a pain in the ass to request from)
            val url =
              new URL(String.format("http://www.bing.com/?q=%s", titleQuery))
            val urlCon = url.openConnection()
            urlCon.setConnectTimeout(2000)
            urlCon.setReadTimeout( 2000 )
            val content =
              fromInputStream( urlCon.getInputStream ).getLines.mkString("\n")

            Console.printf("Retrieved URL '%s' with size: %d\n",
              url.toString, content.size)

            // Assuming things haven't gone south by here
            // let's extract some content (stupidly)
            val linkPat = """(?siu)<li><a (.*?)>(.*?)</a>""".r
            val linkMatches = linkPat.findAllIn(content).matchData
            // drop the first 8 and last 12 b/c they're boilerplate
            val linkData = linkMatches.toList.map(_.group(2)).drop(8).dropRight(12)
            val linkPhrases = linkData.map {
              l => """(?s)<.*?>""".r.replaceAllIn(l, "")
            }
            val linkTerms = linkPhrases.map(_.split(" ")).flatten.toSet
            val filtered = linkTerms.filterNot(t => stopwords(t.toLowerCase))
            returnValue = filtered.size.toDouble / linkTerms.size.toDouble
          }
        }
        returnValue
      }
      feats += f
      feats.toList
    }

    // Query prep
    val query = "new york city"
    val queryNodes = bowNodes(query, "extents")
    val index = Sources.get('aquaint)
    val auxIndex = Sources.get('gov2)
    val lengths = index.getLengthsIterator
    val nodeMap = LinkedHashMap[Node, java.lang.Object]()
    for (n <- queryNodes) { nodeMap.update(n,
      index.getIterator(n).asInstanceOf[TEI])
    }

    val iterators = nodeMap.values.toList.map(obj2movableIt(_))

    // Make the scorers - each one will have its own feature/weight vector
    val scorers = queryNodes.map { qN =>
      val scorer = dirichlet(collectionFrequency(qN, index, lengths))
      val features = generateFeatures(qN, nodeMap, index, auxIndex)
      val weights = List.fill(features.size)(1.0)
      ParameterizedScorer(features, weights, scorer, lengths, nodeMap(qN))
    }

    // Scoring loop
    val resultQueue = standardScoringLoop(scorers, iterators, lengths)

    // Get doc names and print
    printResults(resultQueue, index)
  }

  models("kldiv") = () => {

    // Hack for now to load some common definitions
    import GalagoBridging._

    // let's load some stopwords
    val stopwords = Stopwords.inquery

    val index = Sources.get('aquaint)
    val dummyParams = new Parameters()
    val queryDoc = index.getDocument("NYT19980609.0018", dummyParams)

    // formulate a query from the document
    // let's...start by getting the headline terms
    val headlineTerms =
      """(?s)<HEADLINE>(.*)</HEADLINE>""".r.findFirstMatchIn(queryDoc.text) match {
        case Some(m) => {
          val group = m.group(1)
          var terms = group.replaceAll("""\s""", " ").split(" ").toList
          terms = terms.filterNot(stopwords(_))
          terms = terms.filter(_.size > 1)
          terms.map(_.toLowerCase)
        }
        case None => List.empty
      }

    // pull all quotes in the document, (we're going to turn them into
    // unordered windows, but remove punctuation and stopwords in the groups)
    val quotes =
      """(?s)``(.*?)''""".r.findAllIn(queryDoc.text).matchData.toList.map { m =>
        // Extract the matched group
        m.group(1)
      } map {
        // Scrub as one whole string
        scrub(_)
      } map {
        // And split on WS and make sets (remove dupes)
        _.split("\\s").toSet
      } map {
        // remove any stopwords, short words, and numbers
        _.filterNot(s => stopwords(s) || s.size <= 1 || isAllDigits(s))
      } filter { set : Set[String] =>
        // And keep non-empty sets
        set.size > 0
      } map {
        // And finally turn each set into a list
        _.toList
      }

    // Finally, let's pull all pairs of consecutive words that are capitalized
    // and pretend they're names :)
    val namePattern = """(?s)[A-Z][a-z]+\s(Mc)?[A-Z][a-z]+""".r
    val namesMatches = namePattern.findAllIn(queryDoc.text).toSet
    val names = namesMatches.map { s =>
      s.replaceAll("""\s""", " ").toLowerCase
    } map { name : String =>
      // Make pairs of words
      name.split(" ")
    }

    // Going to need the lengths and term iterators
    val lengths = index.getLengthsIterator
    val allterms = (headlineTerms ++ quotes.flatten ++ names.flatten).toSet
    val nodeMap = LinkedHashMap[Node, java.lang.Object]()
    val termMap = LinkedHashMap[String, Node]()
    // Gonna be naive here and just load extents for everyone - have to look at
    // dependencies for that later
    for (t <- allterms) {
      val n = formatNode(t, "extents")
      val iter = index.getIterator(n)
      termMap += (t -> n)
      nodeMap += (n -> iter)
    }

    // Make unigrams from the headline words, (dirichlet)
    val headlineScorers = headlineTerms.map { t =>
      val scorer = dirichlet(collectionFrequency(termMap(t), index, lengths))
      val ps = ParameterizedScorer(scorer, lengths, nodeMap(termMap(t)))
      ps.weight = 1.2 // why not?
      ps
    }
    // unordered windows from the quotes, (JM)
    val quoteScorers = quotes.map { listOfTerms =>
      val scorer = jm(collectionFrequency(termMap(listOfTerms(0)), index, lengths))
      val isect = uw(listOfTerms.size + 4)
      val iterators =
        listOfTerms.map(t => nodeMap(termMap(t)).asInstanceOf[TEI])
      val ps = ParameterizedScorer(scorer, isect, lengths, iterators)
      ps.weight = 0.35
      ps
    }
    // and od-1's of the names (BM25)
    val nd = numDocuments(lengths).toDouble
    val adl = avgLength(lengths)
    val nameScorers = names.map { firstlast =>
      val idf = nd / (documentCount(termMap(firstlast(0)), index) + 0.5)
      val scorer = bm25(adl, idf)
      val isect = od(1)
      val iterators = firstlast.map(t => nodeMap(termMap(t)).asInstanceOf[TEI])
      val ps = ParameterizedScorer(scorer, isect, lengths, iterators.toList)
      ps.weight = 0.66
      ps
    }

    val allScorers = headlineScorers ++ quoteScorers ++ nameScorers
    val iterators = nodeMap.values.toList.map(_.asInstanceOf[MovableIterator])
    val initialResults = standardScoringLoop(allScorers, iterators, lengths, 1000)

    // Have top 1K - now rerank using a smoothed KL-Divergence
    // unfortunately - that means parsing with the lame TagTokenizer
    val tokenizer = new TagTokenizer()
    val docs = initialResults.map { sd =>
      val d = index.getDocument(index.getName(sd.docid), dummyParams)
      if (d.terms == null || d.terms.size == 0) tokenizer.tokenize(d)
      d // tokenize method returns void/Unit
    }

    // Now make probability distributions
    val probDists = docs.map(multinomial(_))

    // Have to do it w/ the ref document
    val queryDist = multinomial(queryDoc).probs

    // and now score those with KL-Divergence (and sort)
    val cl = collectionLength(lengths)
    val finalScoredDocs = probDists.map { candidate =>
      val keys = queryDist.keys
      val kldiv = keys.map { t =>
        val p = queryDist(t)
        val q = candidate.probs.getOrElse(t, 1.0 / cl)
        scala.math.log(p / q) * p
      }.sum
      ScoredDocument(candidate.id, kldiv)
    }.toList.sorted(ScoredDocumentOrdering).take(100)

    // Get doc names and print
    printResults(finalScoredDocs, index)
  }

  models("relmodel") = () => {
    // Query prep
    val query = "new york city"
    val queryNodes = bowNodes(query)

    val index = Sources.get('aquaint)
    val lengths = index.getLengthsIterator
    val nodeMap = LinkedHashMap[Node, java.lang.Object]()
    for (n <- queryNodes) { nodeMap.update(n,
      index.getIterator(n).asInstanceOf[PositionIndexReader#TermCountIterator])
    }

    val iterators = nodeMap.values.toList.map(obj2movableIt(_))

    // These are the LM scorers - will need them later
    val lmScorers = unigrams(nodeMap, index, lengths)

    // Scoring loop for first pass
    var resultQueue = standardScoringLoop(lmScorers, iterators, lengths)
    val fbDocs = 10  // make proper variable later

    // Trickier part - set up for 2nd run

    // take fbDocs
    var initialResults = ListBuffer[ScoredDocument]()
    while (initialResults.size < fbDocs && !resultQueue.isEmpty)
      initialResults += resultQueue.dequeue

    // now recover "probabilities" of each doc
    val max = initialResults.map(_.score).max
    val logSumExp = max + scala.math.log(initialResults.map { v =>
      scala.math.exp(v.score - max)
    }.sum)
    initialResults = initialResults.map { sd =>
      ScoredDocument(sd.docid, scala.math.exp(sd.score - logSumExp))
    }

    // get the actual documents, and count the grams
    import org.lemurproject.galago.core.parse.Document
    import org.lemurproject.galago.core.parse.TagTokenizer
    val tokenizer = new TagTokenizer()
    val dummy = new Parameters()
    // load and tokenize docs
    val docs = initialResults.map { SD =>
      val d = index.getDocument(index.getName(SD.docid), dummy)
      if (d.terms == null || d.terms.size == 0) tokenizer.tokenize(d)
      d // tokenize method returns void/Unit
    }

    // Get stopwords to filter
    val stopwords = Stopwords.inquery

    // histograms of the # of occurrences - each doc is a histogram
    val hists = docs.map( d => (d.identifier, histogram(d)) ).toMap

    // Set of fb terms
    var terms = hists.values map(_.counts.keySet) reduceLeft { (A,B) => A ++ B }
    // that are NOT stopwords
    terms = terms.filterNot(stopwords(_))
    // that are NOT 1-character or less
    terms = terms.filterNot(_.size <= 1)
    // and are NOT all digits
    terms = terms.filterNot(isAllDigits(_))

    // Apparently we need lengths too. Makes (docid -> length) map
    val doclengths = initialResults.map(_.docid).map {
      A => (A, index.getLength(A))
    }.toMap

    // Time to score the terms
    val grams = terms.map { T =>
      // map to score-per-doc then sum
      val score = initialResults.map { SD =>
        val tf =
          hists(SD.docid).counts.getOrElse(T, 0).toDouble / doclengths(SD.docid)
        SD.score * tf
      }.sum
      Gram(T, score)
    }

    // Sort and keep top "fbTerms"
    val fbTerms = 20
    // selectedGrams = term -> Gram map
    val selectedGrams =
      grams.toList.sorted(GramOrdering).take(fbTerms).map(g => (g.term, g)).toMap

    // Need to open new iterators - only open the ones we don't already have
    // rmNodes = term -> Node map
    val rmNodes =
      selectedGrams.values.map { g =>
        (g.term, formatNode(g.term, "counts"))
      } filterNot { pair => nodeMap.contains(pair._2) }

    val rmNodeMap = LinkedHashMap[Node, java.lang.Object]()
    for ((term, node) <- rmNodes) { rmNodeMap.update(node,
      index.getIterator(node).asInstanceOf[PositionIndexReader#TermCountIterator])
    }

    val rmScorers = rmNodes.map { case (term, node) =>
        val scorer = dirichlet(collectionFrequency(node, index, lengths))
        val ps = ParameterizedScorer(scorer, lengths, rmNodeMap(node))
        ps.weight = selectedGrams(term).score
        ps
    }.toList

    val finalScorers = List(
      ParameterizedScorer(lmScorers, 0.7),
      ParameterizedScorer(rmScorers, 0.3)
    )

    val finalIterators = (rmNodeMap.values.toList.map(obj2movableIt(_)) ++
      iterators).toSet.toList
    // Make sure everything's ready to go
    finalIterators.foreach(_.reset)
    lengths.reset

    // Scoring loop
    resultQueue = standardScoringLoop(finalScorers, finalIterators, lengths)

    // Get doc names and print
    printResults(resultQueue, index)
  }
}
